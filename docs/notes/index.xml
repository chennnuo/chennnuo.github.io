<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes on My Docs</title><link>https://chennnuo.github.io/docs/notes/</link><description>Recent content in Notes on My Docs</description><generator>Hugo</generator><language>en</language><copyright>Copyright (c) 2020-2024 Thulite</copyright><lastBuildDate>Mon, 18 Nov 2024 14:20:00 +0800</lastBuildDate><atom:link href="https://chennnuo.github.io/docs/notes/index.xml" rel="self" type="application/rss+xml"/><item><title>What is Attention</title><link>https://chennnuo.github.io/docs/notes/what-is-attention/</link><pubDate>Mon, 18 Nov 2024 14:20:00 +0800</pubDate><guid>https://chennnuo.github.io/docs/notes/what-is-attention/</guid><description>0. 参考资料 Video 动画形式直观解释注意力机制 1. Attention机制概述 Attention机制是Transformer架构的核心,它使模型能够动态地关注输入序列中的不同部分。这种机制允许模型捕捉长距离依赖关系,这在处理长序列时特别重要。
2. Self-Attention的基本概念 Self-Attention的核心思想是,序列中的每个元素都应该关注整个序列,包括自身。它通过以下三个关键概念实现:
Query (查询): 当前我们正在处理的元素 Key (键): 用于与Query进行比较的元素 Value (值): 实际被聚合的信息 以NLP任务为例，Value 是前面这些词最终想要加给下一个单词的的偏移量，目标是得到 Q &amp;ndash;&amp;gt; V 的映射，这个映射主要用来压缩前后文信息，加到当前word上。</description></item></channel></rss>